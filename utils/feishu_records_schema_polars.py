# encoding='utf-8

# @Time: 2025-08-29
# @File: %
#!/usr/bin/env
# -*- coding: utf-8 -*-
# Feishu x Polars helpers (URL æŸ¥è¯¢ + æ‰å¹³åŒ– + å›å†™)
# --------------------------------------------------
# ä¾èµ–:
#   pip install lark-oapi polars
#
# ä¸»è¦åŠŸèƒ½:
#   - build_client: åˆ›å»ºé£ä¹¦å®¢æˆ·ç«¯
#   - parse_app_and_table_from_url: ä»å¤šç»´è¡¨æ ¼ URL è§£æ app_token/table_id
#   - fetch_field_schema: æ‹‰å–å­—æ®µ schemaï¼ˆç”¨äºç±»å‹ä¸å­—æ®µID->åç§°æ˜ å°„ï¼‰
#   - search_records: ä¼ ç»ŸæŒ‰ token/table_id ç¿»é¡µæŸ¥è¯¢
#   - search_records_by_url: ğŸ”¥ æ–°å¢ï¼ŒæŒ‰ URL + æ¡ä»¶ï¼ˆå«æ—¥æœŸ ExactDateï¼‰ç¿»é¡µæŸ¥è¯¢
#   - search_records_by_url_to_polars: ğŸ”¥ æ–°å¢ï¼Œç›´æ¥è¿”å› Polars DataFrame
#   - records_to_polars_by_schema: å°† records æ‰å¹³åŒ–ä¸º DataFrame
#   - polars_to_records_by_schema: DataFrame è¿˜åŸå› recordsï¼ˆæŒ‰ schemaï¼‰
#   - df_to_feishu_records: å°† DataFrame ç²—è½¬ä¸º recordsï¼ˆå®½æ¾ç‰ˆï¼‰
#   - insert_records_to_feishu: æ‰¹é‡å†™å…¥
#
# æ³¨æ„:
#   - æ—¥æœŸç­›é€‰éµå¾ªå®˜æ–¹â€œè®°å½•ç­›é€‰æŒ‡å—â€ï¼šDateTime å­—æ®µç”¨ ["ExactDate","<æ¯«ç§’å­—ç¬¦ä¸²>"]
#   - å…¬å¼æ—¥æœŸå­—æ®µç”¨ ["ExactDate","yyyy/MM/dd"]
#   - value ä¸€å¾‹æ˜¯ã€Œåˆ—è¡¨ã€ï¼›æ¯«ç§’éœ€æ˜¯ã€Œå­—ç¬¦ä¸²ã€

# -*- coding: utf-8 -*-
"""
Feishu Bitable â†” Polars Utilities (URL search with ExactDate & list support)

åŠŸèƒ½æ¦‚è¦
- build_client: åˆ›å»ºé£ä¹¦ lark-oapi å®¢æˆ·ç«¯
- parse_app_and_table_from_url: ä» URL è§£æ app_token/table_id/view_id
- fetch_field_schema: è·å–å­—æ®µ schemaï¼ˆå«å­—æ®µIDâ†’åç§°æ˜ å°„ï¼‰
- records_to_polars_by_schema / polars_to_records_by_schema
- df_to_feishu_records: å®½æ¾çš„ DataFrame -> records
- insert_records_to_feishu: æ‰¹é‡å†™å…¥
- search_records: ä¼ ç»ŸæŒ‰ token/table_id æœç´¢
- search_records_by_url: âœ… æŒ‰ URL æœç´¢ï¼Œæ”¯æŒ date_eq ä¸ºå•å€¼æˆ–åˆ—è¡¨ï¼Œä¸¥æ ¼æŒ‰å®˜æ–¹è¯·æ±‚ç»“æ„æ„é€ 
- search_records_by_url_to_polars: ä¾¿æ·å°è£…ï¼Œç›´æ¥è¿”å› Polars DataFrame
"""
from __future__ import annotations

import ast
import json
import re
import time
import datetime
from typing import Any, Dict, List, Sequence, Optional, Iterable, Literal, Tuple

import polars as pl
import lark_oapi as lark
from zoneinfo import ZoneInfo
from urllib.parse import urlparse, parse_qs

from lark_oapi.api.bitable.v1 import (
    ListAppTableFieldRequest,
    SearchAppTableRecordRequest,
    SearchAppTableRecordRequestBody,
    BatchCreateAppTableRecordRequest,
    BatchCreateAppTableRecordRequestBody,
    BatchCreateAppTableRecordResponse,
    FilterInfo,
    Condition,
)

# ======================= 1) å®¢æˆ·ç«¯ & URL è§£æ =======================

def build_client(app_id: str, app_secret: str) -> lark.Client:
    assert isinstance(app_id, str) and isinstance(app_secret, str)
    return (
        lark.Client.builder()
        .app_id(app_id)
        .app_secret(app_secret)
        .log_level(lark.LogLevel.INFO)
        .build()
    )

def parse_app_and_table_from_url(table_url: str) -> Tuple[str, str, Optional[str]]:
    """ä»å¤šç»´è¡¨æ ¼ URL è§£æ (app_token, table_id, view_id)"""
    if isinstance(table_url, (tuple, list)):
        table_url = table_url[0]
    m = re.search(r"/base/([A-Za-z0-9]+)", table_url)
    if not m:
        raise ValueError("æ— æ³•ä» URL ä¸­è§£æ app_token")
    app_token = m.group(1)

    q = parse_qs(urlparse(table_url).query)
    table_id = q.get("table", [None])[0]
    if not table_id:
        raise ValueError("æ— æ³•ä» URL ä¸­è§£æ table_id")
    view_id = q.get("view", [None])[0]
    return app_token, table_id, view_id

# ======================= 2) å­—æ®µ schema =======================

TYPE_CODE_TO_NAME = {
    1: "text",
    2: "number",
    3: "single_select",
    4: "multi_select",
    5: "date",       # Date/DateTime ç»Ÿä¸€å« date
    7: "checkbox",
}

def normalize_type_name(type_value: Any) -> str:
    if isinstance(type_value, int):
        return TYPE_CODE_TO_NAME.get(type_value, "unknown")
    if isinstance(type_value, str):
        return type_value.strip().lower()
    return "unknown"

def fetch_field_schema(
    client: lark.Client,
    app_token: str,
    table_id: str,
    page_size: int = 100,
    max_pages: int = 20,
) -> Dict[str, Dict[str, Any]]:
    """
    è¿”å›: { field_name: {"field_id": "...", "type": "<normalized_type>", "raw_type": <åŸå§‹>, "property": {...}} }
    """
    schema: Dict[str, Dict[str, Any]] = {}
    page_token = ""
    seen = set()
    page_no = 0

    while True:
        page_no += 1
        builder = (
            ListAppTableFieldRequest.builder()
            .app_token(app_token)
            .table_id(table_id)
            .page_size(page_size)
        )
        if page_token:
            builder = builder.page_token(page_token)

        resp = client.bitable.v1.app_table_field.list(builder.build())
        if not resp.success():
            if resp.code == 1254030 and page_token:
                print(f"[warn] InvalidPageToken {page_token!r}, restart")
                page_token = ""
                continue
            raise RuntimeError(f"æ‹‰å–å­—æ®µå¤±è´¥: code={resp.code}, msg={resp.msg}")

        data = resp.data
        items = getattr(data, "items", None) or []
        for it in items:
            field_name = getattr(it, "field_name", None) or getattr(it, "name", None)
            field_id   = getattr(it, "field_id", None)   or getattr(it, "id", None)
            raw_type   = getattr(it, "type", None)
            prop       = getattr(it, "property", None)
            tname = normalize_type_name(raw_type)
            if field_name:
                schema[field_name] = {
                    "field_id": field_id,
                    "type": tname,
                    "raw_type": raw_type,
                    "property": prop,
                }

        next_token = getattr(data, "page_token", "") or ""
        if not next_token:
            break
        if next_token in seen:
            print(f"[warn] repeated page_token {next_token!r}, stop")
            break
        seen.add(next_token)
        page_token = next_token
        if page_no >= max_pages:
            print(f"[warn] reached max_pages={max_pages}, stop")
            break
    return schema

# ======================= 3) records -> Polars =======================

def records_to_polars_by_schema(
    records: Sequence[Dict[str, Any]],
    field_schema: Dict[str, Dict[str, Any]] | None,
    keep_record_id: bool = True,
    list_text_strategy: Literal["first", "join"] = "first",
    join_sep: str = ";",
) -> pl.DataFrame:
    def _first_text_from_list(lst: list[Any]) -> Optional[str]:
        for x in lst:
            if isinstance(x, dict) and x.get("text") is not None:
                return str(x["text"])
            if isinstance(x, (str, int, float, bool)):
                return str(x)
        return None

    def _all_texts_from_list(lst: list[Any]) -> List[str]:
        out: List[str] = []
        for x in lst:
            if isinstance(x, dict) and x.get("text") is not None:
                out.append(str(x["text"]))
            elif isinstance(x, (str, int, float, bool)):
                out.append(str(x))
        return out

    def _extract_textish(value: Any) -> Optional[str]:
        if value is None:
            return None
        if isinstance(value, (str, int, float, bool)):
            return str(value)
        if isinstance(value, list):
            if list_text_strategy == "first":
                return _first_text_from_list(value)
            all_txt = _all_texts_from_list(value)
            return join_sep.join(all_txt) if all_txt else None
        if isinstance(value, dict):
            if value.get("text") is not None:
                return str(value["text"])
            if isinstance(value.get("value"), list):
                if list_text_strategy == "first":
                    return _first_text_from_list(value["value"])
                all_txt = _all_texts_from_list(value["value"])
                return join_sep.join(all_txt) if all_txt else None
            for k in ("name", "title", "label"):
                if value.get(k) is not None:
                    return str(value[k])
            return str(value)
        return str(value)

    def _flatten_value_by_type(val: Any, tname: str) -> Any:
        if val is None:
            return None
        tname = (tname or "").lower()
        if tname == "number":
            try:
                return float(val)
            except Exception:
                txt = _extract_textish(val)
                try:
                    return float(txt) if txt is not None else None
                except Exception:
                    return None
        if tname == "checkbox":
            if isinstance(val, bool):
                return val
            txt = _extract_textish(val)
            if txt is None:
                return None
            return str(txt).strip().lower() in {"1", "true", "yes", "y", "t"}
        return _extract_textish(val)

    if field_schema:
        all_fields = list(field_schema.keys())
        types_map = {k: (field_schema[k].get("type") or "") for k in all_fields}
    else:
        seen = set()
        for rec in records:
            f = rec.get("fields", {}) or {}
            for k in f.keys():
                seen.add(k)
        all_fields = list(seen)
        types_map = {k: "text" for k in all_fields}

    rows: List[Dict[str, Any]] = []
    for rec in records:
        row: Dict[str, Any] = {}
        if keep_record_id:
            row["record_id"] = rec.get("record_id")
        f = rec.get("fields", {}) or {}
        for name in all_fields:
            tname = types_map.get(name, "text")
            row[name] = _flatten_value_by_type(f.get(name), tname)
        for k, v in rec.items():
            if k not in {"fields", "record_id"} and k not in row:
                row[k] = v
        rows.append(row)

    df = pl.DataFrame(rows)
    for name in all_fields:
        if name not in df.columns:
            continue
        tname = (types_map.get(name) or "").lower()
        try:
            if tname == "number":
                df = df.with_columns(pl.col(name).cast(pl.Float64, strict=False))
            elif tname == "checkbox":
                df = df.with_columns(pl.col(name).cast(pl.Boolean, strict=False))
            else:
                df = df.with_columns(pl.col(name).cast(pl.Utf8, strict=False))
        except Exception:
            pass
    if keep_record_id and "record_id" in df.columns:
        df = df.with_columns(pl.col("record_id").cast(pl.Utf8, strict=False))
    return df

# ======================= 4) Polars -> records =======================

def _restore_value_by_type(val: Any, tname: str) -> Any:
    if val is None:
        return None
    if tname == "text":
        s = "" if val is None else str(val)
        return [] if s == "" else [{"type": "text", "text": s}]
    if tname == "number":
        try:
            num = float(val)
            return int(num) if float(num).is_integer() else num
        except Exception:
            return str(val)
    if tname == "checkbox":
        return bool(val)
    if tname in ("date",):
        return str(val)
    if tname in ("single_select", "multi_select"):
        try:
            return json.loads(val)
        except Exception:
            return val
    try:
        return json.loads(val)
    except Exception:
        return val

def polars_to_records_by_schema(
    df: pl.DataFrame,
    field_schema: Dict[str, Dict[str, Any]],
    keep_record_id: bool = True,
) -> List[Dict[str, Any]]:
    out: List[Dict[str, Any]] = []
    all_fields = list(field_schema.keys())
    for row in df.iter_rows(named=True):
        fields: Dict[str, Any] = {}
        for name in all_fields:
            if name not in row:
                continue
            tname = field_schema[name]["type"]
            restored = _restore_value_by_type(row[name], tname)
            if restored is None or restored == []:
                continue
            fields[name] = restored
        rec: Dict[str, Any] = {"fields": fields}
        if keep_record_id and ("record_id" in df.columns):
            rid = row.get("record_id")
            if rid:
                rec["record_id"] = rid
        out.append(rec)
    return out

# ======================= 5) å®½æ¾ç‰ˆ DF -> records =======================

def df_to_feishu_records(df: pl.DataFrame) -> list[dict]:
    def to_timestamp(v):
        if isinstance(v, (datetime.date, datetime.datetime)):
            return int(time.mktime(v.timetuple()) * 1000)
        elif isinstance(v, str):
            for fmt in ("%Y-%m-%d", "%Y/%m/%d", "%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                try:
                    dt = datetime.datetime.strptime(v, fmt)
                    return int(time.mktime(dt.timetuple()) * 1000)
                except Exception:
                    continue
            return v
        return v

    records = []
    for row in df.to_dicts():
        fields = {}
        for k, v in row.items():
            if v is None:
                fields[k] = None
            elif isinstance(v, (datetime.date, datetime.datetime)):
                fields[k] = to_timestamp(v)
            elif isinstance(v, str) and re.match(r"^\d{4}[-/]\d{1,2}[-/]\d{1,2}", v):
                fields[k] = to_timestamp(v)
            elif isinstance(v, float):
                fields[k] = int(v) if v.is_integer() else v
            else:
                fields[k] = v
        records.append({"fields": fields})
    return records

# ======================= 6) æ‰¹é‡å†™å…¥ =======================

def insert_records_to_feishu(app_id: str, app_secret: str, table_url: str, records: list[dict]):
    app_token, table_id, _ = parse_app_and_table_from_url(table_url)
    client = (
        lark.Client.builder()
        .app_id(app_id)
        .app_secret(app_secret)
        .log_level(lark.LogLevel.DEBUG)
        .build()
    )
    request: BatchCreateAppTableRecordRequest = (
        BatchCreateAppTableRecordRequest.builder()
        .app_token(app_token)
        .table_id(table_id)
        .request_body(
            BatchCreateAppTableRecordRequestBody.builder()
            .records(records)
            .build()
        )
        .build()
    )
    response: BatchCreateAppTableRecordResponse = client.bitable.v1.app_table_record.batch_create(request)
    if not response.success():
        lark.logger.error(
            f"batch_create failed, code: {response.code}, msg: {response.msg}, log_id: {response.get_log_id()}, "
            f"resp:\n{json.dumps(json.loads(response.raw.content), indent=4, ensure_ascii=False)}"
        )
        return None
    lark.logger.info(lark.JSON.marshal(response.data, indent=4))
    return response.data

# ======================= 7) ä¼ ç»ŸæŒ‰ token/table æŸ¥è¯¢ =======================

def search_records(
    client: lark.Client,
    app_token: str,
    table_id: str,
    *,
    view_id: str | None = None,
    page_size: int = 500,
    max_pages: int = 50,
    field_schema: dict[str, dict] | None = None,
) -> list[dict]:
    id2name = {}
    if field_schema:
        id2name = {meta.get("field_id"): name for name, meta in field_schema.items() if meta.get("field_id")}
    items: list[dict] = []
    page_token = ""
    seen = set()
    page_no = 0
    while True:
        page_no += 1
        req_builder = (
            SearchAppTableRecordRequest.builder()
            .app_token(app_token)
            .table_id(table_id)
            .page_size(page_size)
        )
        if view_id:
            req_builder = req_builder.view_id(view_id)
        if page_token:
            req_builder = req_builder.page_token(page_token)

        body = SearchAppTableRecordRequestBody.builder().build()
        resp = client.bitable.v1.app_table_record.search(req_builder.request_body(body).build())
        if not resp.success():
            if resp.code == 1254030 and page_token:
                print(f"[warn] InvalidPageToken {page_token!r}, restart")
                page_token = ""
                continue
            raise RuntimeError(f"æŸ¥è¯¢è®°å½•å¤±è´¥: code={resp.code}, msg={resp.msg}")

        raw_list = getattr(resp.data, "items", []) or []
        for it in raw_list:
            rec_id  = getattr(it, "record_id", None) if not isinstance(it, dict) else it.get("record_id")
            fields  = getattr(it, "fields", {}) if not isinstance(it, dict) else (it.get("fields") or {}) or {}
            if isinstance(fields, dict) and any(isinstance(k, str) and k.startswith("fld") for k in fields.keys()):
                fields = {id2name.get(k, k): v for k, v in fields.items()}
            items.append({"record_id": rec_id, "fields": fields})

        next_token = getattr(resp.data, "page_token", "") or ""
        has_more   = getattr(resp.data, "has_more", None)
        if not next_token or has_more is False:
            break
        if next_token in seen:
            print(f"[warn] repeated page_token {next_token!r}, stop")
            break
        if page_no >= max_pages:
            print(f"[warn] reached max_pages={max_pages}, stop")
            break
        seen.add(next_token)
        page_token = next_token
    return items

# ======================= 8) URL æœç´¢ï¼ˆæ”¯æŒ date_eq åˆ—è¡¨ï¼‰ =======================

def _date_str_to_midnight_ts_ms(date_str: str, tz: str = "Asia/Shanghai") -> int:
    """
    å°† 'YYYY-MM-DD' æˆ– 'YYYY/MM/DD' è½¬ä¸ºè¯¥æ—¶åŒºå½“å¤© 00:00:00 çš„æ¯«ç§’æ—¶é—´æˆ³
    """
    s = str(date_str).replace("/", "-").strip()
    dt = datetime.datetime.strptime(s, "%Y-%m-%d").replace(
        tzinfo=ZoneInfo(tz), hour=0, minute=0, second=0, microsecond=0
    )
    return int(dt.timestamp() * 1000)

def _coerce_date_eq_list(date_eq: str | int | Iterable | None) -> List[Any]:
    """
    ç»Ÿä¸€æŠŠ date_eq è½¬ä¸ºåˆ—è¡¨ï¼š
    - æ ‡é‡ -> [æ ‡é‡]
    - list/tuple -> list(...)
    - åƒåˆ—è¡¨çš„å­—ç¬¦ä¸²ï¼ˆ"['2025-08-21','2025-08-22']"ï¼‰-> è§£æä¸ºåˆ—è¡¨
    """
    if date_eq is None:
        return []
    if isinstance(date_eq, (list, tuple)):
        return list(date_eq)
    if isinstance(date_eq, str):
        s = date_eq.strip()
        if (s.startswith("[") and s.endswith("]")) or (s.startswith("(") and s.endswith(")")):
            # å…ˆå°è¯• JSONï¼Œå†ç”¨ literal_eval å…œåº•
            try:
                return json.loads(s)
            except Exception:
                try:
                    v = ast.literal_eval(s)
                    return list(v) if isinstance(v, (list, tuple)) else [date_eq]
                except Exception:
                    return [date_eq]
        return [date_eq]
    return [date_eq]

def _exactdate_value_from_item(
    item: Any,
    *,
    is_formula_date: bool,
    doc_tz: str,
) -> List[str]:
    """
    å•ä¸ªæ—¥æœŸé¡¹ -> å®˜æ–¹ ExactDate å€¼ï¼š
    - DateTime å­—æ®µ: ["ExactDate", "<æ¯«ç§’å­—ç¬¦ä¸²>"]
    - å…¬å¼æ—¥æœŸå­—æ®µ: ["ExactDate", "yyyy/MM/dd"]
    """
    if is_formula_date:
        # å…è®¸ä¼ æ¯«ç§’ï¼šè½¬æˆæ–‡æ¡£æ—¶åŒºçš„ yyyy/MM/dd
        if isinstance(item, (int, float)) or (isinstance(item, str) and item.isdigit()):
            ts_ms = int(item)
            dt = datetime.datetime.fromtimestamp(ts_ms / 1000.0, tz=ZoneInfo(doc_tz))
            return ["ExactDate", dt.strftime("%Y/%m/%d")]
        return ["ExactDate", str(item).replace("-", "/")]

    # DateTime å­—æ®µï¼šéœ€è¦æ¯«ç§’å­—ç¬¦ä¸²
    if isinstance(item, (int, float)) or (isinstance(item, str) and item.isdigit()):
        return ["ExactDate", str(int(item))]
    ts_ms = _date_str_to_midnight_ts_ms(str(item), tz=doc_tz)
    return ["ExactDate", str(ts_ms)]

def search_records_by_url(
    table_url: str,
    app_id: str,
    app_secret: str,
    *,
    view_id: str | None = None,
    user_id_type: str = "user_id",
    page_size: int = 20,          # å®˜æ–¹ç¤ºä¾‹é»˜è®¤ 20ï¼Œå¯è‡ªè¡Œè°ƒå¤§
    max_pages: int = 50,
    # æ—¥æœŸç­‰å€¼ç­›é€‰ï¼ˆå½“æ—¥ï¼‰
    date_field_name: str | None = None,
    date_eq: str | int | Iterable | None = None,   # âœ… æ”¯æŒåˆ—è¡¨æˆ–åˆ—è¡¨å­—ç¬¦ä¸²
    date_is_formula: bool = False,                 # True=å…¬å¼æ—¥æœŸï¼›False=DateTime
    doc_tz: str = "Asia/Shanghai",
    # å…¶ä»–é™„åŠ æ¡ä»¶ï¼ˆè¦æ±‚å·²æ„é€ å¥½ value ä¸ºå­—ç¬¦ä¸²æ•°ç»„æˆ–å¯è½¬ä¸ºå­—ç¬¦ä¸²çš„å…ƒç´ åˆ—è¡¨ï¼‰
    extra_conditions: Iterable[dict] | None = None,
    automatic_fields: bool = False,
    map_field_id_to_name: bool = True,
) -> list[dict]:
    """
    æŒ‰å®˜æ–¹è¯·æ±‚ç»“æ„ï¼Œé€šè¿‡ URL æœç´¢ï¼ˆæ”¯æŒ date_eq ä¸ºåˆ—è¡¨ï¼›å¤šæ—¥æœŸå°†åˆ†åˆ«æŸ¥è¯¢å†åˆå¹¶å»é‡ï¼‰
    """
    app_token, table_id, view_in_url = parse_app_and_table_from_url(table_url)
    if view_id is None:
        view_id = view_in_url

    client = build_client(app_id, app_secret)

    # å¯é€‰ï¼šå­—æ®µ ID -> åç§°
    id2name = {}
    if map_field_id_to_name:
        try:
            schema = fetch_field_schema(client, app_token, table_id)
            id2name = {meta.get("field_id"): name for name, meta in schema.items() if meta.get("field_id")}
        except Exception:
            id2name = {}

    # ç»Ÿä¸€æ—¥æœŸä¸ºåˆ—è¡¨ï¼ˆå¯èƒ½ä¸ºç©ºè¡¨ç¤ºä¸åŠ æ—¥æœŸæ¡ä»¶ï¼‰
    date_items = _coerce_date_eq_list(date_eq) if (date_field_name and date_eq is not None) else [None]

    def _run_one_query(one_date_item: Any) -> list[dict]:
        # ç»„è£… Filterï¼ˆä¸¥æ ¼å¯¹é½å®˜æ–¹ï¼švalue ä¸ºã€Œå­—ç¬¦ä¸²æ•°ç»„ã€ï¼‰
        cond_list = []

        if date_field_name and one_date_item is not None:
            exact_val = _exactdate_value_from_item(
                one_date_item, is_formula_date=date_is_formula, doc_tz=doc_tz
            )
            exact_val = [str(exact_val[0]), str(exact_val[1])]
            cond_list.append(
                Condition.builder()
                    .field_name(date_field_name)
                    .operator("is")
                    .value(exact_val)
                    .build()
            )

        if extra_conditions:
            for c in extra_conditions:
                rawv = c.get("value") or []
                # ç»Ÿä¸€è½¬å­—ç¬¦ä¸²
                val = [str(v) for v in rawv]
                cond_list.append(
                    Condition.builder()
                        .field_name(c["field_name"])
                        .operator(c["operator"])
                        .value(val)
                        .build()
                )

        filter_obj = None
        if cond_list:
            filter_obj = FilterInfo.builder().conjunction("and").conditions(cond_list).build()

        # ç¿»é¡µ
        items: list[dict] = []
        page_token = ""
        seen = set()
        page_no = 0

        while True:
            page_no += 1
            body_b = SearchAppTableRecordRequestBody.builder()
            if view_id:
                body_b = body_b.view_id(view_id)
            if filter_obj is not None:
                body_b = body_b.filter(filter_obj)
            body_b = body_b.automatic_fields(automatic_fields)
            body = body_b.build()

            req_b = (
                SearchAppTableRecordRequest.builder()
                .app_token(app_token)
                .table_id(table_id)
                .user_id_type(user_id_type)
                .page_size(page_size)
            )
            if page_token:
                req_b = req_b.page_token(page_token)

            resp = client.bitable.v1.app_table_record.search(req_b.request_body(body).build())
            if not resp.success():
                raise RuntimeError(
                    f"search failed, code={resp.code}, msg={resp.msg}, log_id={resp.get_log_id()}, "
                    f"resp={resp.raw.content if getattr(resp, 'raw', None) else ''}"
                )

            raw_list = getattr(resp.data, "items", []) or []
            for it in raw_list:
                rec_id = getattr(it, "record_id", None) if not isinstance(it, dict) else it.get("record_id")
                fields = getattr(it, "fields", {}) if not isinstance(it, dict) else (it.get("fields") or {})
                if isinstance(fields, dict) and any(isinstance(k, str) and k.startswith("fld") for k in fields.keys()):
                    fields = {id2name.get(k, k): v for k, v in fields.items()}
                items.append({"record_id": rec_id, "fields": fields})

            next_token = getattr(resp.data, "page_token", "") or ""
            has_more = getattr(resp.data, "has_more", None)

            if not next_token or has_more is False:
                break
            if next_token in seen or page_no >= max_pages:
                break
            seen.add(next_token)
            page_token = next_token
        return items

    # å¤šæ—¥æœŸï¼šåˆ†åˆ«æŸ¥è¯¢ï¼Œä¸å…¶ä»–æ¡ä»¶åš ANDï¼Œæœ€åæŒ‰ record_id åˆå¹¶å¹¶é›†
    merged: dict[str, dict] = {}
    for di in date_items:
        for rec in _run_one_query(di):
            rid = rec.get("record_id") or f"__noid__{id(rec)}"
            if rid not in merged:
                merged[rid] = rec
    return list(merged.values())

def search_records_by_url_to_polars(
    table_url: str,
    app_id: str,
    app_secret: str,
    *,
    view_id: str | None = None,
    user_id_type: str = "user_id",
    page_size: int = 20,
    max_pages: int = 50,
    date_field_name: str | None = None,
    date_eq: str | int | Iterable | None = None,  # âœ… åŒæ­¥æ”¯æŒåˆ—è¡¨
    date_is_formula: bool = False,
    doc_tz: str = "Asia/Shanghai",
    extra_conditions: Iterable[dict] | None = None,
    automatic_fields: bool = False,
) -> pl.DataFrame:
    """
    ä¾¿æ·å°è£…ï¼šæŒ‰ URL æœç´¢ï¼ˆæ”¯æŒ date_eq åˆ—è¡¨ï¼‰ï¼Œå¹¶ç›´æ¥è¿”å› Polars DataFrame
    """
    app_token, table_id, _ = parse_app_and_table_from_url(table_url)
    client = build_client(app_id, app_secret)
    field_schema = fetch_field_schema(client, app_token, table_id)

    records = search_records_by_url(
        table_url, app_id, app_secret,
        view_id=view_id,
        user_id_type=user_id_type,
        page_size=page_size,
        max_pages=max_pages,
        date_field_name=date_field_name,
        date_eq=date_eq,
        date_is_formula=date_is_formula,
        doc_tz=doc_tz,
        extra_conditions=extra_conditions,
        automatic_fields=automatic_fields,
        map_field_id_to_name=True,
    )
    return records_to_polars_by_schema(records, field_schema)

# ======================= 9) ç¤ºä¾‹ =======================

if __name__ == "__main__":
    # è¯·æ›¿æ¢ä¸ºä½ çš„çœŸå®ä¿¡æ¯
    exit()
    APP_ID = "cli_a819ae6445685013"
    APP_SECRET = "WZVSbtc80PYSJjDr8CHZDgcbILzKgzW0"
    TABLE_URL = "https://xcn114pn5b7h.feishu.cn/base/N5cFb1e6Za8gShsw6gnc7FWYnod?table=tbl5FH77jwAWnm4S&view=vew5TWuGAM"
    #
    # # ç¤ºä¾‹1ï¼šå•æ—¥æœŸï¼ˆDateTime å­—æ®µï¼‰
    # try:
    #     recs = search_records_by_url(
    #         TABLE_URL, APP_ID, APP_SECRET,
    #         view_id="vew5TWuGAM",
    #         # user_id_type="user_id",
    #         date_field_name="æ—¥æœŸ",
    #         date_eq="2025-08-21",     # ä¹Ÿå¯ "2025/08/21" æˆ– 1755705600000
    #         date_is_formula=False,    # DateTime å­—æ®µ
    #         # doc_tz="Asia/Seoul",
    #         page_size=20,
    #     )
    #     print(f"[single] records: {len(recs)}")
    # except Exception as e:
    #     print("Error(single):", e)
    #
    # # exit()
    # # ç¤ºä¾‹2ï¼šå¤šæ—¥æœŸåˆ—è¡¨ï¼ˆAND å…¶ä»–æ¡ä»¶ï¼ŒOR æ—¥æœŸå¹¶é›†ï¼‰
    # try:
    #     recs2 = search_records_by_url(
    #         TABLE_URL, APP_ID, APP_SECRET,
    #         view_id="vew5TWuGAM",
    #         date_field_name="æ—¥æœŸ",
    #         date_eq=["2025-08-21", "2025-08-20"],  # âœ… ç›´æ¥åˆ—è¡¨
    #         date_is_formula=False,
    #         # doc_tz="Asia/Seoul",
    #         extra_conditions=[
    #             # ç¤ºä¾‹ï¼šèŒä½ = åˆçº§é”€å”®å‘˜
    #             # {"field_name": "èŒä½", "operator": "is", "value": ["åˆçº§é”€å”®å‘˜"]}
    #         ],
    #         page_size=200,
    #     )
    #     print(f"[multi] records: {len(recs2)}")
    # except Exception as e:
    #     print("Error(multi):", e)

    # ç¤ºä¾‹3ï¼šç›´æ¥æ‹¿ DataFrameï¼ˆåŒæ ·æ”¯æŒ date_eq åˆ—è¡¨ï¼‰
    try:
        df = search_records_by_url_to_polars(
            TABLE_URL, APP_ID, APP_SECRET,
            # view_id="vew5TWuGAM",
            # user_id_type="user_id",
            date_field_name="æ—¥æœŸ",
            date_eq="['2025-08-21','2025-08-26']",  # âœ… åˆ—è¡¨å­—ç¬¦ä¸²ä¹ŸOK
            date_is_formula=False,
            # doc_tz="Asia/Seoul",
            # page_size=20,
        )
        print(len(df))
        print(df.head())
    except Exception as e:
        print("Error(df):", e)
